# -*- coding: utf-8 -*-
"""Copia_di_ActorCritic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOuFTz1RSFp6YZo5ZxDdA33qJzQ1C-DE
"""

#!pip install git+https://github.com/Kojoley/atari-py.git

import gym
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from weights_manager import weights_manager
from reinforce_agent import reinforce_agent
from montecarlo import montecarlo_rollout
from video_simulation import pacman_video
from PIL import Image


EPISODES = 10000
EVALUATION = 50

value_path_prefix = "value-weights"
policy_path_prefix = "policy-weights"
path_suffix = ".h5"
value_path = value_path_prefix + path_suffix
policy_path = policy_path_prefix + path_suffix

#Create and init env
env= gym.make("Boxing-v0")
env.reset()

#Getting actions space and action meanings
actions_list = env.action_space
action_space_dim = len(env.env.get_action_meanings())
print(actions_list)
print(env.env.get_action_meanings())


#Print observation sample
observation, _, _, _ = env.step(env.action_space.sample())
print("Showing a sample observation")
plt.imshow(observation)
plt.show()
print("Observation shape registered: {}".format(observation.shape))

#Preprocessing function for the observation reduce dim and to grayscale
def rgb2gray(rgb):
  r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]
  gray = 0.2989 * r + 0.5870 * g + 0.1140 * b

  return gray

#Reshape the observation image to reduce the input dimension of the network
def observation_preprocessing(observation):
  img = observation[20:180:2, 20:140:2]
  img = (img - 128) / 128-1
  img = rgb2gray(img)
  return np.expand_dims(img.reshape(80,60), axis=2)


plt.imshow(observation_preprocessing(observation)[:,:,0])
plt.show()
print("Observation preprocessed shape: {}".format(observation_preprocessing(observation).shape))


pacman_agent = reinforce_agent()
#Check for pretrained model weights to import

manager = weights_manager(value_path_prefix, policy_path_prefix, path_suffix)
weights_found = manager.check_pretrained_weights(pacman_agent)
if weights_found:
  print("Evaluating current model average reward")
  pacman_agent.top_avg_reward = pacman_agent.evaluate_model(env)

#Training the network
for e in range( EPISODES):
  print(f"Episode: {e} start!")
  e_steps, total_reward = montecarlo_rollout(pacman_agent, env)
  v_loss, p_loss = pacman_agent.training(e_steps, total_reward)
  #Saving new models weights every 25 episodes
  if e % EVALUATION == 0 and e > 0:
    if pacman_agent.model_compare(env, e):
      value_path = value_path_prefix + str(e) + path_suffix
      pacman_agent.v_model.save_weights(value_path)
      policy_path = policy_path_prefix + str(e) + path_suffix
      pacman_agent.p_model.save_weights(policy_path)

video = pacman_video(action_space_dim, policy_path)
video.create_video()