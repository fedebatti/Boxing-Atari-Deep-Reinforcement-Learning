{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MS_Pacman_2_0_4xFrames.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dflp8V72l1Zi"
      },
      "source": [
        "#MS PACMAN with deep Q Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzpGRHepl-ON"
      },
      "source": [
        "Imports and action list print after creation of the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0_pC_ZVGxP8"
      },
      "source": [
        "!pip install git+https://github.com/Kojoley/atari-py.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnLBa6TmeuIC"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from collections import deque, Counter\n",
        "from skimage import color\n",
        "from skimage import io\n",
        "import IPython\n",
        "from pathlib import Path\n",
        "import imageio\n",
        "import base64\n",
        "from PIL import Image\n",
        "\n",
        "#Create and initialize the environment\n",
        "env = gym.make(\"Boxing-v0\")\n",
        "env.reset()\n",
        "#Getting actions space and action meanings\n",
        "actions_list = env.action_space\n",
        "print(actions_list)\n",
        "print(env.env.get_action_meanings())\n",
        "\n",
        "#Testing observations\n",
        "observation, _, _, _ = env.step(env.action_space.sample())\n",
        "print(\"Showing a sample observation\")\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "\n",
        "print(\"Observation shape registered: {}\".format(observation.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciQzWC_G1Eh3"
      },
      "source": [
        "Observation preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYgZujYlyDPc"
      },
      "source": [
        "def rgb2gray(rgb):\n",
        "  r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
        "  gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "\n",
        "  return gray\n",
        "\n",
        "#Reshape the observation image to reduce the input dimension of the network\n",
        "def observation_preprocessing(observation):\n",
        "  img = observation[20:180:2, 20:140:2]\n",
        "  # img = (img - 128) / 128-1\n",
        "  img = rgb2gray(img)\n",
        "  return np.expand_dims(img.reshape(80,60), axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJBqKBNCyuSv"
      },
      "source": [
        "plt.imshow(observation_preprocessing(observation)[:,:,0])\n",
        "plt.show()\n",
        "print(\"Observation preprocessing shape: {}\".format(observation_preprocessing(observation).shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiVKIjcvpMRv"
      },
      "source": [
        "Model initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WuWNQ4FT8FE"
      },
      "source": [
        "Hyperparameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0iwOuOvT_0D"
      },
      "source": [
        "num_episodes = 2000\n",
        "maximum_steps = 3000\n",
        "steps_train = 4\n",
        "start_steps = 2000\n",
        "window_dim = 4\n",
        "\n",
        "\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "avg_best_reward = -1000\n",
        "\n",
        "discount_factor = 0.95\n",
        "\n",
        "scores_file = 'scores_history.txt'\n",
        "\n",
        "#Network parameters\n",
        "batch_size = 32\n",
        "optimizer = tf.keras.optimizers.Adam(lr=1e-6)\n",
        "loss_fn = tf.keras.losses.mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t2GQAbCUG9T"
      },
      "source": [
        "Creation of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nom8du6lwjGK"
      },
      "source": [
        "#Try to add padding='SAME'\n",
        "def create_model(input_shape, output_shape):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(filters=16, kernel_size=(8,8), strides=4, padding='SAME', activation='relu'))\n",
        "  model.add(layers.Conv2D(filters=32, kernel_size=(4,4), strides=2, padding='SAME', activation='relu'))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256))\n",
        "  model.add(layers.Dense(output_shape))\n",
        "\n",
        "  model.build(input_shape=input_shape)\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jot0Gn_XUT63"
      },
      "source": [
        "Building the model (training and target)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMQwHUSR_3a-"
      },
      "source": [
        "#Defining input shape and output shape of the model\n",
        "input_shape = (None, 80, 60, 4)\n",
        "output_shape = len(env.env.get_action_meanings())\n",
        "\n",
        "#Let's build our Q-Networks\n",
        "training_network = create_model(input_shape, output_shape)\n",
        "target_network = create_model(input_shape, output_shape)\n",
        "\n",
        "training_network.compile(optimizer=optimizer,\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy', 'loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgjkuDugUt6X"
      },
      "source": [
        "Self play functions initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq2McNNuUxeb"
      },
      "source": [
        "#Self play parameters\n",
        "replay_buffer_len = 10000\n",
        "\n",
        "#Buffer is made from a deque â€” double ended queue\n",
        "replay_buffer = deque(maxlen=replay_buffer_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0LwcXfk7kTw"
      },
      "source": [
        "#Epsilon greedy policy function definition\n",
        "def epsilon_greedy_policy(state_array, model, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "      return np.random.randint(env.action_space.n)\n",
        "    else:\n",
        "      Q_values = model.predict(state_array)\n",
        "      return np.argmax(Q_values)\n",
        "\n",
        "#Greedy policy function definition\n",
        "def select_greedy_action(state_array, model):\n",
        "  Q_values = model.predict(state_array)\n",
        "  return np.argmax(Q_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23aOoQH0bmAZ"
      },
      "source": [
        "#Create the input tensor with 4 frame in succession\n",
        "def create_input_tensor(x):\n",
        "  x = np.array(x)\n",
        "  return np.expand_dims(x.reshape((80,60,4)), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmcG-Dt_WzOP"
      },
      "source": [
        "#Let the agent play a single step\n",
        "def play_one_step(env, state, epsilon, model, step, simulation=False):\n",
        "  #Used for simulation, greedy action selection required\n",
        "  if simulation is True:\n",
        "    #Greedy action for simulation\n",
        "    action = select_greedy_action(create_input_tensor(state), model)\n",
        "  else:\n",
        "    #Epsilon Greedy for training\n",
        "    action = epsilon_greedy_policy(create_input_tensor(state), model, epsilon)\n",
        "  next_obs, reward, done, info = env.step(action)\n",
        "  next_state = state.copy()\n",
        "  next_state.pop(0)\n",
        "  next_state.append(observation_preprocessing(next_obs))\n",
        "  state_tensor = create_input_tensor(state)\n",
        "  next_state_tensor = create_input_tensor(next_state)\n",
        "  replay_buffer.append((state_tensor, action, reward, next_state_tensor, done))\n",
        "  return next_obs, reward, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l56_v4scWsgA"
      },
      "source": [
        "###Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTc_mnNYXyI4"
      },
      "source": [
        "Sampling function to sample states among self-played games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R04KCbA5-CXy"
      },
      "source": [
        "#Sample a batch of states\n",
        "def sample_states(batch_size):\n",
        "  indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "  batch = [replay_buffer[index] for index in indices]\n",
        "  states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(5)]\n",
        "  return states, actions, rewards, next_states, dones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0hqwCqhWr8B"
      },
      "source": [
        "#Single training step\n",
        "def training_step(batch_size, episode, print_loss=False):\n",
        "  #Sampling experiences\n",
        "  experiences = sample_states(batch_size)\n",
        "  states, actions, rewards, next_states, dones = experiences\n",
        "  #Predicting next states Q values\n",
        "  next_Q_values = training_network.predict(next_states)\n",
        "  #print(next_Q_values)\n",
        "  max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "  target_Q_values = (rewards + (1-dones)*discount_factor*max_next_Q_values)\n",
        "  mask = tf.one_hot(actions, output_shape)\n",
        "  with tf.GradientTape() as tape:\n",
        "      all_Q_values = training_network(states)\n",
        "      Q_values = tf.reduce_sum(all_Q_values*mask, axis=1, keepdims=True)\n",
        "      loss = tf.reduce_mean(loss_fn(target_Q_values,Q_values))\n",
        "      if print_loss:\n",
        "        print(\"Loss value: {}, in episode {}\".format(loss, episode))\n",
        "  grads = tape.gradient(loss, training_network.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, training_network.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lj5HjG_Wx09"
      },
      "source": [
        "Self play function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jZQz1g5I57F"
      },
      "source": [
        "#Play full episode\n",
        "def play_episode(env, model):\n",
        "  #States array thinking of implementing a network which takes in input multiple states\n",
        "  reward_accumulator = 0\n",
        "  obs = env.reset()\n",
        "  state = deque(maxlen=4)\n",
        "  while len(state) < 4:\n",
        "    state.append(observation_preprocessing(obs))\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "    state_tensor = create_input_tensor(list(state))\n",
        "    action = select_greedy_action(state_tensor, model)\n",
        "\n",
        "    # Take action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state.append(observation_preprocessing(obs))\n",
        "    reward_accumulator += reward\n",
        "    \n",
        "  print(\"Total episode reward: {}\".format(reward_accumulator))\n",
        "  return state, reward, reward_accumulator\n",
        "\n",
        "#Function to evaluate the current training model against the best performant model until now\n",
        "def evaluate_model(model, env):\n",
        "  num_episodes = 10\n",
        "  reward_list = []\n",
        "  #print(\"Playing with the last training's model\")\n",
        "  for ep in range(num_episodes):\n",
        "  #  print(\"Starting simulation for episode {}\".format(ep))\n",
        "    _, _, reward = play_episode(env, model)\n",
        "    reward_list.append(reward)\n",
        "  \n",
        "  return np.mean(np.array(reward_list)) \n",
        "\n",
        "#Compare new model's result with the last best model result\n",
        "def model_compare(model1, environment, episode):\n",
        "  print(\"Evaluating training model\")\n",
        "  training_model_value = evaluate_model(model1, env)\n",
        "  global avg_best_reward\n",
        "  with open(scores_file, \"a\") as scores_f:\n",
        "    scores_f.write(f\"\\nepisode:{episode} average_score:{training_model_value}\")\n",
        "  if training_model_value > avg_best_reward:\n",
        "    print(\"New best model found\")\n",
        "    avg_best_reward = training_model_value\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tahgbgvcbf9_"
      },
      "source": [
        "#Check for pretrained model weights to import\n",
        "weight_path_prefix = \"training-weights\"\n",
        "weight_path_suffix = \".h5\"\n",
        "weight_path = weight_path_prefix + weight_path_suffix\n",
        "global avg_best_reward\n",
        "print(\"Checking for pretrained model weights\")\n",
        "if Path(weight_path).exists() and Path(weight_path).is_file():\n",
        "  print(\"Existing weights file found, loading files on the training model\")\n",
        "  training_network.load_weights(weight_path)\n",
        "  target_network.load_weights(weight_path)\n",
        "  print(\"Successfully loaded weights\")\n",
        "  print(\"Computing current best model average reward\")\n",
        "  avg_best_reward = evaluate_model(training_network, env)\n",
        "else:\n",
        "  print(\"Pretrained model not found starting training from the beginning\")\n",
        "  print(\"Set current best model average reward to 0\")\n",
        "  avg_best_reward = -1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw-ON9B6w7nQ"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k1fyFCdW2uL"
      },
      "source": [
        "#Training the model\n",
        "for episode in range(516, num_episodes):\n",
        "    print(\"Episode: {} start\".format(episode))\n",
        "    obs = env.reset()\n",
        "    state = deque(maxlen=4)\n",
        "    while len(state) < 4:\n",
        "      state.append(observation_preprocessing(obs))\n",
        "    #Saving weight every 25 episodes\n",
        "    if episode % 25 == 0 and episode >50:\n",
        "      if model_compare(training_network, env, episode):\n",
        "        weight_path = weight_path_prefix + str(episode) + weight_path_suffix\n",
        "        training_network.save_weights(weight_path)\n",
        "        target_network.load_weights(weight_path)\n",
        "    \n",
        "    for step in range(maximum_steps):\n",
        "      # print(f\"step {step} {np.array(state).shape}\")\n",
        "      epsilon = max(1- episode/10000, 0.01)\n",
        "      obs, reward, done, info = play_one_step(env, list(state), epsilon, training_network, step)\n",
        "      state.append(observation_preprocessing(obs))\n",
        "      if done:\n",
        "          break\n",
        "      if episode > 50:\n",
        "       if step % 2 ==0:\n",
        "        if step % 200 == 0:\n",
        "          training_step(batch_size, episode, print_loss=True)\n",
        "        else:\n",
        "          training_step(batch_size, episode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73_npbbjp-Ho"
      },
      "source": [
        "#Generate self play video to evaluate the performances of the system\n",
        "def create_video(env, model, video_filename = 'imageio'):\n",
        "  def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "\n",
        "  num_episodes = 5\n",
        "  print(\"Setting video name\")\n",
        "  video_filename = video_filename + \".mp4\"\n",
        "  with imageio.get_writer(video_filename, fps=60) as video:\n",
        "    for ep in range(num_episodes):\n",
        "        print(\"Starting simulation for episode {}\".format(ep))\n",
        "        state = deque(maxlen=4)\n",
        "        obs = env.reset()\n",
        "        while len(state) < 4:\n",
        "          state.append(observation_preprocessing(obs))\n",
        "\n",
        "        terminated = False\n",
        "        while not terminated:\n",
        "            #agent_states = np.expand_dims(np.array(states).reshape((80,80,4)), axis=0)\n",
        "            action = select_greedy_action(create_input_tensor(state), model)\n",
        "\n",
        "            # Take action\n",
        "            obs, _, terminated, _ = env.step(action)\n",
        "            state.append(observation_preprocessing(obs))\n",
        "            video.append_data(obs)\n",
        "  print(\"Simulation completed video is ready to be downloaded\")\n",
        "  embed_mp4(video_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlA8cJcA2rxk"
      },
      "source": [
        "#Simulation\n",
        "#training_network.load_weights(weight_path)\n",
        "create_video(env, training_network)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}